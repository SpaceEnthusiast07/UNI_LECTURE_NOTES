[Slides](https://ele.exeter.ac.uk/pluginfile.php/5410528/mod_resource/content/0/COM1011-L05-Hypothesis-Testing.pdf)


##### Contents
 - [[#What is Hypothesis Testing]]
 - [[#The Hypothesis]]
 - [[#p-Value]]
 - [[#Calculating a p-value (Example)]]
 - [[#z-Values]]
 - [[#t-Statistic]]
 - [[#From t-Value to p-Value]]
 - [[#Student's t-Test]]
 - [[#Practical Tools Mentioned]]
 - [[#Key Takeaways]]
 - [[#How this connects with ML]]


##### What is Hypothesis Testing
 - ***Hypothesis testing*** is a statistical method used to decide whether an *observed difference* is *real* or *random*
 - Example:
	 - You test Drug A and Drug B for curing a disease
	 - Drug A cures 30%
	 - Drug B cures 42%
	 - Is B actually better, or did random chance cause the difference?
	 - Hypothesis testing is used to find this out!


##### The Hypothesis
 - ***Null Hypothesis*** ($H_0$)
	 - There is *no difference* between the two groups
	 - E.g. "Drug A and Drug B are equally as effective"
 - ***Alternative Hypothesis*** ($H_1$)
	 - There is *a difference* between the groups
	 - E.g. "Drug B is more effective than Drug A"
 - You test whether there is *enough evidence* to *reject* $H_0$ and *accept* $H_1$


##### p-Value
 - The ***p-value*** measures how likely your observed results occurred by *random chance* if the *null hypothesis is true*
 - Small $p$-value ($p < 0.05$) $\rightarrow$ Unlikely due to chance $\rightarrow$ *Reject* $H_0$ and accept $H_1$
 - Large $p$-value ($p > 0.05$) $\rightarrow$ Could be due to chance $\rightarrow$ *Accept* $H_0$
 - *Threshold* ($\alpha$): Usually $0.05$ ($5\%$)
 - If $p < 0.05$ $\rightarrow$ result is considered ***statistically significant***
 - *Understanding*:
	 - If $p = 0.001$ $\rightarrow$ Only $0.1\%$ chance results came from randomness $\rightarrow$ *significant difference* $\rightarrow$ *Reject* $H_0$ and accept $H_1$
	 - If $p = 0.24$ $\rightarrow$ $24\%$ chance results came from randomness $\rightarrow$ *not significant* $\rightarrow$ *Accept* $H_0$


##### Calculating a p-value (Example)
 - Imagine 5 professors on a panel, all over 65 years old.
 - At the University, only $20\%$ of all professors are over 65.
 - Hypotheses:
	 - $H_0$: Panel is chosen *randomly*
	 - $H_1$: Panel is chosen *deliberately*
 - The probability ($p$-value) that all 5 are over 65 = $0.2^5 = 0.00032 = 0.03\%$
 - Since $p < 0.05$, *reject* $H_0$ and *accept* $H_1$, concluding that the panel was chosen *deliberately*


##### z-Values
 - When we know the population mean ($\mu$) and standard deviation ($\sigma$), we use a $z$-score to measure how far the data point ($x$) is from the mean ($\mu$):
$$z\:score: \;\; z = \dfrac{x - \mu}{\sigma}$$
 - *Large* $|z|$ means the result is *extreme* or unusual
 - $z$-values help find $p$-values under the *normal distribution*

>[!tip] From *A-Level Maths*, the $z$-score is simply *transforming* the normal distribution to the *standard normal*, with $\mu = 0$ and $\sigma = 1$!


##### t-Statistic
 - When we *don't know the true* $\mu$ or $\sigma$ (which is common), we *estimate* them from a *sample* using a *t-statistic*:
$$t = \dfrac{Observed \; Effect}{Standard Error}$$
 - Numerator $\rightarrow$ how big the observed effect is
 - Denominator $\rightarrow$ how noisy the estimate is
 - The *larger* $t$ is, the stronger the evidence against $H_0$ is


##### From t-Value to p-Value
 - The $t$-value can be converted into the $p$-value using the $t$-distribution
 - Assumptions:
	 - Data is *approximately* normally distributed
	 - Samples are *independent*
	 - Variances are *similar*
 - Even if the assumptions are *slightly off*, $t$-tests are *usually robust* (meaning they still work seasonably well)


##### Student's t-Test
 - Used to *compare the means* of two samples and check if they are *significantly different*
 - Example applications:
	 - Compare two drugs (clinical testing)
	 - Compare two machine learning models (e.g. accuracy, F1 score)
 - In Machine Learning:
	 - If you test two classifiers (such as a perceptron vs logistic reg.) using cross-validation:
	 - You would use ...![[Pasted image 20251107171913.png]]
	 - ... to check if their performance difference is statistically significant


##### Practical Tools Mentioned
 - `scipy.stats.ttest_rel()` - performs a paired $t$-test
 - `mlxtend library` - has functions like `paired_ttest_5x2cv()` for comparing classifiers


##### Key Takeaways
![[Pasted image 20251107172159.png]]


##### How this connects with ML
 - These concepts *help determine* whether one model *truly performs better* than another
 - Or if the difference in metrics (accuracy, F1, etc.) is just random variation in the test data
 - Hypothesis testing ensures your performance claims are statistically reliable, not just based on luck


