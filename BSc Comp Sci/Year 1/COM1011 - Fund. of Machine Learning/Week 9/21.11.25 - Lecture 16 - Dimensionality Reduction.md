[Slides](https://ele.exeter.ac.uk/pluginfile.php/5300373/mod_resource/content/7/COM1011-L14-DimensionalityReduction.pdf)


##### Contents
 - [[#Why does Dimensionality Reduction Matter?]]
 - [[#Two Dimensionality Reduction Strategies]]
	 - [[#Feature Selection (keep originals, but fewer)]]
	 - [[#Feature Extraction (create new features)]]
 - [[#Principle Component Analysis (PCA)]]
	 - 


##### Why does Dimensionality Reduction Matter?
 - High-dimensional datasets (many columns/features) cause problems
 - Reasons to reduce dimensionality:
	 - *Noise removal* $\to$ Get rid of irrelevant or redundant features
	 - *Efficiency* $\to$ Fewer dimensions $=$ faster training
	 - *Better generalisation* $\to$ Reduces overfitting by removing useless features
	 - *Visualisation* $\to$ Humans can only see 2D or 3D. Techniques like PCA, t-SNE, and UMAP make high-D data plottable


#### Two Dimensionality Reduction Strategies
##### Feature Selection (keep originals, but fewer)
 - You delete or keep select features
 - There are 3 families of feature selection:
 1. *Filter Method*
	 - Use stats to decide which features seen useful
	 - Example: *Variance Threshold*
		 - Features with very low variance (almost constant) contain little information
		 - Steps:
			 1. Compute variance of each feature
			 2. Drop features with variance $<$ threshold
			 3. Must normalise first (so scales are comparable)
	 - *Variance* $=$ how much values spread out. A feature like "ID Number" has near-zero variance $\to$ useless
 2. *Wrapper Methods*
	 - Try many models with different feature subsets, pick the best-performing
	 - ***Forward Search***
		 1. Start with *no features*, add one at a time
		 2. At each step pick the feature the improve performance most
	 - ***Recursive Feature Elimination (RFE)***
		 1. Start will all features, remove the least important one each time
	 - Pros: accurate
	 - Cons: expensive (many model evaluations)
 3. *Embedded Methods*
	 - The model itself decides feature important
	 - Example: *Decision Trees*
		 - Trees naturally pick the best features at each split
		 - They use criteria:
			 - *Gini impurity* $\to$ measures how mixed the classes are
			 - *Information gain* $\to$ reduction in uncertainty
			 - *Variance reduction* $\to$ for regression trees
		 - Low impurity $=$ good split
		 - High information gain $=$ good split
	 - Trees are:
		 - Transparent
		 - Easy to interpret
		 - Good at feature selection (but can overfit)


##### Feature Extraction (create new features)
 - Instead of choosing existing features, we create brand new ones by transforming the data
 - The goal is to create a smaller set of *informative combinations* of features


#### Principle Component Analysis (PCA)
##### Intuition
***PCA*** finds new axes of the data that capture the *most variation*
 - These new axes $=$ *principle components*
 - PC1 captures the most variance
 - PC2 captures the second most
 - PCs are orthogonal (uncorrelated)
 - PCA is basically a *rotation + flip* of the data (you rotate the cloud of points so the axes line up with its shape)


##### Why Variance?
 - Features that vary more usually encode more information


##### How PCA Works (conceptually)
 - Compute covariance matrix
	 - Covariance measure how two variables change together
 - Compute eigenvectors and eigenvalues of covariance matrix
	 - Eigenvectors $=$ directions of PCs
	 - Eigenvalues $=$ amount of variance each PC captures
 - Transform data
	 - $Y = XV$ (rotate points into the PC system)
 - Keep the first $K$ components as your reduced data
