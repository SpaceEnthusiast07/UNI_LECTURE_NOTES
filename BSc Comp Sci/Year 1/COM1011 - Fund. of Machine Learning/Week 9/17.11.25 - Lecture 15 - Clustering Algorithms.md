[Slides](https://ele.exeter.ac.uk/pluginfile.php/5300372/mod_resource/content/7/COM1011-L13-KMeans-GMM-ClusteringMeasures.pdf)


##### Contents
 - [[#Partitional Clustering]]
 - [[#K-means Clustering]]
	 - [[#The Big Idea]]


##### Partitional Clustering
 - These algorithms directly divide space into $K$ *non-overlapping clusters*
 - *Key Idea*:
	 - Instead of comparing *every point to every other point* $\rightarrow O(N^2)$ 
	 - Compare each point to $K$ *centroids* $\rightarrow O(NK)$
 - Much faster, especially for large datasets
 - The main algorithm here is $K$-means


#### K-means Clustering
##### The Big Idea
 - $K$-means tries to find $K$ clusters, each represented by its ***centroid*** - the centre point of the cluster
 - The Algorithm:
	 1. *Input*: Choose the number of clusters $K$
	 2. *Initialise*: Pick $K$ starting centroid (often chosen randomly)
	 3. *Assignment*: Assign each point to the nearest centroid
	 4. *Update*: Recompute each centroid as the mean of its assigned points
	 5. *Repeat* steps $3$-$4$ until the centroid stop changing (i.e. they have converged on their correct place)
 - Convergence happens when:
	 - Almost no points switch cluster
	 - Centroids barley change
	 - $SSE$ (Sum of Squared Errors) stops decreasing


##### K-means Implementation
```python
import sklearn.cluster import KMeans

# Initialise the algorithm with the num of clusters
kmeans = KMeans(n_clusters=3)

# Create the clusters based on a dataset
labels = kmeans.fit_predict(X)
```


##### Pros of K-means
 - Fast: runtime roughly proportional to $O(n*d*k*t)$
	 - *(points x dimensions x clusters x iterations)*
 - Easy to understand and implement
 - Works well when clusters are:
	 - similar in size
	 - roughly spherical
	 - not too many outliers


##### Cons of K-means
 - $K$-means has several important weaknesses
 1. You must choose $K$ - No automatic selection of number of clusters
 2. Only works is a "centroid" makes sense - Bad for categorical data without modification
 3. Sensitive to outliers - A single extreme point can drag a centroid far away
 4. Sensitive to initialisation - Different starting points $\rightarrow$ different final clusters (this is why $K$-means++ is used)
 5. Assumes spherical clusters - Struggles with clusters that have different:
	 - sizes
	 - shapes
	 - densities


##### Fixing K-means
 - Pre-processing
	 - Scale/normalise the data
	 - Remove outliers
 - Post-processing
	 - Remove very small clusters
	 - Merge clusters that are too similar
	 - Split clusters that are too spread out
 - Alternatives
	 - For robustness: use $K$-medians (L1 loss)
	 - For better initialisation: use $K$-means++
	 - For soft clustering/non-spherical shapes: use *GMMs*


##### 


