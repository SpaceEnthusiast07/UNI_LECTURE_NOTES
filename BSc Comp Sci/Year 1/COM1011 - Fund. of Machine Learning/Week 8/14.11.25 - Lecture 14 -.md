Slides


##### Contents
 - [[#How do we evaluate clustering?]]
 - [[#Slime-Mould Optimisation]]
 - [[#Part 1 - Hierarchical Clustering]]
	 - [[#Hierarchical Clustering]]
	 - [[#Agglomerative (Bottom-Up) Hierarchical Clustering]]
	 - [[#Why not try all possible dendrograms?]]
	 - [[#How do we measure distance between clusters?]]


##### How do we evaluate clustering?
 - *Clustering* is a form of *unsupervised learning* meaning that you don't have *ground truth tables* for the data, so evaluation is *tricky*
 - Methods to evaluate clustering:
	 - *Internal Metrics*:
		 - These metrics look at the compactness and separation of clusters
		 - Silhouette score
		 - Davies-Bouldin index
		 - Reconstruction error, log-likelihood, ELBO (for probabilistic models)
	 - *Stability Tests*:
		 - Run the model multiple times (different seeds or data subsamples)
		 - If the clusters keep changing $\rightarrow$ the algorithm is unstable
	 - *Human Evaluation*:
		 - If the task is subjective (e.g. "good" image groups), humans may assess cluster quality


##### Slime-Mould Optimisation
![[Pasted image 20251114105853.png]]


#### Part 1 - Hierarchical Clustering
##### Hierarchical Clustering
 - ***Hierarchical clustering*** creates a *tree* (*dendrogram*) showing how data groups at different similarity levels
 - Why use it?
	 - It shows multiple levels of similarity (clusters of clusters)
	 - It *doesn't* require you to specify the number of clusters upfront
	 - it produces a *dendrogram* - a tree that represents the entire clustering hierarchy
		 - Leaves = individual data points
		 - Internal nodes = merged clusters


##### Agglomerative (Bottom-Up) Hierarchical Clustering
 - This is the most common type
 - The Idea:
	 1. Start with every data point as its own cluster
	 2. Compute distances between all clusters
	 3. Merge the two closest clusters
	 4. Update the distance matrix
	 5. Repeat until everything is merged into one big cluster
 - This produces a *dendrogram*
 - Example of a dendrogram: ![[Pasted image 20251114110653.png]]
	 1. Closest point to $A$ is $B$, so group them together
	 2. $E$ and $F$ are close together, so group them
	 3. Group $EF$ is closer to $D$ than $C$, so group $EF$ and $D$
	 4. Now, group $EFD$ is closer to $C$ than $AB$, so group $EFD$ and $C$
	 5. Finally, group $EFDC$ is closer to $AB$ than $Null$, so group $EFDC$ and $AB$
	 - You have now produced a *Dendrogram*
 - Another visual representation of hierarchical clustering: ![[Pasted image 20251114111237.png]]


##### Why not try all possible dendrograms?
 - Because the number of possible dendrograms grows extremely quickly (*combinatorial explosion*)
 - Brute force is impossible


##### How do we measure distance between clusters?
 - This is crucial: Different linkage methods produce different cluster shapes!
 1. Single Linkage (Nearest Neighbour)
	 - Distance = shortest distance between any two points in each cluster
	 - Pros: Can capture weird shapes of clusters
	 - Cons: Can form "chains"; sensitive to noise
 2. Complete Linkage (Farthest Neighbour)
	 - Distance = longest distance between any two points
	 - Pros: Tends to create compact clusters
	 - Cons: Can over-split big clusters
 3. Average Linkage
	 - Distance = average of all pairwise distances
	 - Balanced, but expensive to compute
 4. Centroid Linkage
	 - Distance = distance between cluster centroids
	 - Works best when clusters are roughly spherical
 5. Ward's Method
	 - Merge clusters only if it minimally increases total within-cluster variance
	 - Creates very round, equally-sized clusters


##### Using the dendrogram
 - To choose actual clusters:
	 - Draw a horizontal cut across the dendrogram
	 - The number of branches you cut though = number of clusters
 - Easy and simple cases, but often ambiguous


#### Part 2 - DBSCAN
##### Density-Based Spatial Clustering of Applications with Noise
 - DBSCAN is a powerful clustering algorithm that finds clusters based on density, not distance to centroids
 - *Useful when clusters*:
	 - Are oddly shaped
	 - Are not spherical
	 - Have noise or outliers
 - *Key Idea*:
	 - A cluster is a dense region of points
	 - Points must have enough nearby neighbours
 - *Two hyperparameters*:
	 - ***eps*** - radius of neighbourhood (how close "nearby" is)
	 - ***MinPts*** - minimum neighbours needed for a point to be considered dense


##### Types of points in DBSCAN
 - ***Core point***:
	 - Has at least *MinPts* neighbours within *eps*
	 - It is "inside" a dense region
 - ***Border point***:
	 - Does not have MinPts neighbours
	 - BUT it is close enough to a core point to belong to its cluster
 - ***Noise point***:
	 - Not a core point, not a border point
	 - These are treated as outliers


##### Cluster Connectivity
 - DBSCAN clusters are formed by points that can reach each other though a chain of steps of length $\leq$ eps
 - This lets DBSCAN find arbitrary shapes


##### DBSCAN Strengths
 - Handles noise well
 - Detects clusters of arbitrary shape
 - Works when cluster shapes are messy
 - Doesn't require specifying number of clusters


##### Limitations
 - *eps* and *MinPts* are hard to tune and they interact
 - Doesn't work well when the dataset has *varying densities* (dense + spares areas together)


##### Visual Intuition
 - Changing *eps* slightly (e.g. $9.75$ $\rightarrow$ $9.92$) can drastically change clusters



