[Slides](https://ele.exeter.ac.uk/pluginfile.php/5566039/mod_resource/content/1/ECM1413_25_26_Lecture_05_Processor_Management.pdf)


##### Contents
 - [[#What is Process Scheduling?]]
 - [[#The Core Scheduling Algorithms]]
	 - [[#First Come First Served (FCFS)]]
	 - [[#Round Robin (RR)]]
	 - [[#Shortest Process Next (SPN)]]
	 - [[#Multilevel Queue]]
 - [[#Scheduling Criteria]]
 - [[#Multiprocessor Scheduling (Multiple Cores)]]


##### What is Process Scheduling?
 - Process Scheduling refers to how we allocate CPU cores to processes.
 - I.e. how to mange the transition between the Ready and Running state.
 - The goal is to keep the CPU as busy as possible, doing useful work.


#### The Core Scheduling Algorithms
 - There are four main algorithms used to decide which process should be given CPU time next.


##### First Come First Served (FCFS)
 - This is the simplest scheduling algorithm.
 - Whoever *arrives first* is allowed to *complete their task entirely* before the next process is allowed to begin.
 - This algorithm is ***non pre-emptive***, meaning that once a process starts, it *cannot be interrupted until it is finished*.
 - *Pros*: There is no unnecessary switching between processes, and each process will eventually get CPU time.
 - *The Problem*: If a very long process starts first, all the short processes behind it have to wait a very long time. This is known as the ***Convoy Effect***.
 - *Example*: Like a single queue checkout at a shop, if the person at the front has two full trolleys, the person behind with just a chocolate bar has to wait ages.


##### Round Robin (RR)
 - This is designed for *time sharing* systems.
 - Each process gets a *small unit of CPU time*, called a ***Time Quantum*** (e.g. 10 milliseconds).
 - *Process*: A process runs for the length of the time quantum. It if isn't finished, it is interrupted (pre-empted) and sent to the back of the "Ready" circular queue (process state is changed to Ready) so the next process can have a turn using the CPU.
 - *Example*: Like a teacher giving each student in a circle 30 seconds to speak, and if you haven't finished, you have to wait until the teacher comes back around to you.
 - *Pros*: Distributes resources in a fair manner, and a quick process can pass through relatively quickly.
 - *Cons*: Long average waiting time when processes require multiple time quanta, and performance heavily depends on the length of the time quantum.


##### Shortest Process Next (SPN)
 - This algorithm looks at *all the processes* in the "Ready" queue, *estimates* and picks the one process that will take the *least amount of time to finish*.
 - This algorithm can be *pre-emptive* or *non pre-emptive*, i.e. processes may be interrupted before they are finished or they may not be.
 - *Pros*: It minimises the average time processes spend waiting.
 - *Cons*: It can lead to ***Starvation***. If short processes keep arriving, a long process might never get a turn. Execution time must be estimated.
 - *Example*: At a doctor's office, the doctor sees everyone with a 5 minute prescription request first, while the person needing a 1 hour physical keeps getting pushed back.


##### Multilevel Queue
 - Instead of one big line, the system has several different queues with different priorities.
 - *How it Works*: High priority processes (like moving your mouse) go into a fast queue, while background processes (like a system update) go into a lower priority queue.
 - *Example*: Like an airport with a first class line and an economy line. The staff always clear the first class line before helping anyone in the economy line.
 - There are multiple design choices to be made with multilevel queues:
 - *Mapping Processes to Queues*: You may have specific queues for interactive (processes with lost of I/O), normal (e.g. system services), or batch processes (processes without I/O).
 - *Relative Priority of Queues*:
	 - *Fixed Priority Scheduling*: e.g. interactive (lots of I/O) processes always get priority over batch processes.
	 - *Time Slicing*: e.g. 80% of CPU time is allocated to interactive processes, 10% to normal, and 10% to batch processes.
 - *Scheduling Algorithms within Queues* ![[Pasted image 20260126151936.png]]
	 - E.g. round robin for interactive and normal processes, and first come first served for batch processes.
 - *Pros*: Complex, can accommodate a range of different performance objectives.
 - *Cons*: Complex, difficult to calibrate.


##### Scheduling Criteria
 - There are two categories for 4 other key criteria:
 - ***User-Oriented***:
	 - *Turnaround Time*: The total time from when a process is submitted to when its finished.
	 - *Response Time*: How lone it takes from a request being make until the first response starts (crucial for things like gaming or typing).
 - ***System-Oriented***:
	 - *Throughput*: The number of completed processes per unit time.
	 - *Waiting Time*: The total time a process spends sitting in the "Ready" queue.
	 - *CPU/Processor Utilisation*: The percentage of time that the processor is busy with useful work, aim for 100%.


##### Multiprocessor Scheduling (Multiple Cores)
 - Most modern processors have *more than more core*.
 - A ***core*** being a separate processor, with the ability to carry out *independent processes*.
 - Each core has its own memory, called cache.
 - Cache stores the data that is currently being used by that core.
 - There are two approaches to multicore scheduling:
 - ***A Common Ready Queue***: When a core becomes available, it is assigned a new process from a common queue.
 - ***Private Queues***: When a core becomes available, it is assigned a new process from its own private queue.
 - There are two methods for improving the performance of a multicore processor:
 - ***Load Balancing***:
	 - When using private queues, processes are shuffled between queues to evenly distribute the workload.
	 - On the other hand, when using a common ready queue, load balancing automatically occurs.
 - ***Processor Affinity***:
	 - A process is kept running on the same core to keep the "*cache warm*".
	 - ***Cache Warmth***: When a process runs on core 1, that core stores specific data in its cache. If you move that process to core 2, it has to reload all that data, which is slow.
	 - Cache warmth automatically occurs in private queues.
 - ***Soft Processor Affinity***: Processes are only moved between cores if there is a good reason to, otherwise they remain on the same core.
 - *The Problem*: Load balancing counteracts processor affinity and visa versa, this is why soft processor affinity is used.


