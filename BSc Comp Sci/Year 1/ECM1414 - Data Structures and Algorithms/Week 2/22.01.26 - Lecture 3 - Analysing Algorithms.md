[Slides](https://ele.exeter.ac.uk/pluginfile.php/5558163/mod_resource/content/1/03-AnalysisOfAlgorithms.pdf)


##### Contents
 - [[#Fundamentals of Algorithmic Problem Solving]]
 - [[#Analysis of Algorithms]]
 - [[#Measuring Input Size]]
 - [[#Measuring Time Efficiency]]
 - [[#Basic Operations]]
 - [[#RAM Model of Computation]]
 - [[#Common Growth Functions]]
 - [[#Worst, Best and Average Case]]
 - [[#Worst Case]]
 - [[#Best Case]]
 - [[#Average Case]]


##### Fundamentals of Algorithmic Problem Solving
 - The process of defining an algorithm has the following steps:
	 1. Understand the problem
	 2. Understanding the capabilities of a computational device
	 3. Choose between exact and approximate problem solving
	 4. Deciding on appropriate data structures
		 - "Algorithms + Data Structures = Programs [Wirth 76]"
	 5. Implement any solution, this will help you fulfil step 1
	 6. Improve on your initial solution, can it be made better, generate many solutions
	 7. Choose the most efficient solution to the problem


##### Analysis of Algorithms
 - What does it mean to analyse?
	 - "Strictly speaking analysis is the separation of an intellectual or substantial whole into its constituent parts for individual study." [American Heritage Dictionary]
 - I.e. decompose the solution into its constituent parts, and analyse each part individually.
 - When you investigate an algorithm, you are assessing the efficiency with respect to two resources:
	 - Running time
	 - Memory space
 - The emphasis on these resources is because we can study them in a precise manner using formal frameworks.


##### Measuring Input Size
 - Almost all programs take longer to execute when the input size is larger.
 - However, it is common to have algorithms that require more than one parameter, e.g. Graph algorithms.


##### Measuring Time Efficiency
 - We don't use a standard unit for time measurement, such as seconds or micro seconds, because that is not only based on the algorithm, but also the computer it is executed on, the compiler used, and how we would record the time.
 - Therefore, we could count how many times each operation is executed in the algorithm, but this may be difficult and unnecessary.


##### Basic Operations
 - The standard approach is to identify the basic operations and count how many times they are executed.
 - As a rule of thumb, they are the most time consuming operation in the innermost loop of the algorithm.
 - Examples:
	 - For sorting, the basic operation is the comparison of values.
	 - For matrix multiplication, the basic operations are multiplication and addition.
	 - However, since on most computers, multiplication is more computationally expensive than addition, we only count multiplication.
 - Therefore, the established framework is to *count the number of times the algorithm's basic operations are executed for input of size $n$*, where $n$ is clearly defined.


##### RAM Model of Computation
 - Machine independent algorithms depends upon a hypothetical computer called the ***Random Access Machine (RAM)***.
 - Within this hypothetical computer, simple operations (+, -, \*, /, =) and memory calls all take 1 time step, and we have an infinite amount of memory.
 - Loops and subroutines are not considered simple operations, instead they are the composition of many single-step operations.
 - The time it takes to run through a loop is dependent on the number of iterations, therefore not 1 time step.


##### Common Growth Functions
![[Pasted image 20260122205146.png]]
 - In comp sci, we use the notation $O(n)$ to represent the *Order* of magnitude of an algorithm, with the specific time complexity within brackets.
 - ***Constant Time*** ($O(1)$):
	 - The execution time is independent of the input size.
	 - An example of an operation with a constant time is when accessing an element in an array. No matter the size of the array, to access the first element, it always takes the same amount of time.
	 - Even though the order of magnitude uses $1$, the actual time might be 3 time steps, the point is that it doesn't change when the input size changes.
	 - Under the RAM model, this is the time complexity of all basic operations.
 - ***Logarithmic*** ($O(\log_{2}n)$):
	 - Normally occurs in algorithms that decompose a large problem down into a smaller version.
	 - This time complexity is common in searching algorithms and some tree algorithms.
 - ***Linear*** ($O(n)$):
	 - An algorithm that must pass through all elements in the input size and number of times will yield a linear time complexity.
	 - Time grows linearly with input size.
 - ***Polylogarithmic*** ($O(n^{k}\log_{2}n)$):
	 - This is typical of algorithms that break a problem down into smaller parts, solve the smaller problems, then combine the solutions to obtain the solution to the original problem.
 - ***Quadratic*** ($O(n^2)$):
	 - A subset of polynomial solutions.
	 - Quadratics solutions are still acceptable and are relatively efficient with small to medium problem sizes.
	 - Typical of an algorithm which must compare all pairs of values.
 - ***Cubic*** ($O(n_{3})$):
	 - Not as efficient as *quadratic*, but still polynomial.
	 - *Note: Not all triple nested loops result in cubic time complexity!*
 - ***Polynomial*** ($O(n^k)$):
	 - Any algorithm with time complexity of $O(n^k)$, where $k$ is a constant, is considered polynomial.
 - ***Exponential*** ($O(2^n)$):
	 - Unfortunately, quite a few solutions to practical problems have time complexity in this category.
	 - This is as bad as testing all possible answers to a problem.
	 - When an algorithm falls into this category, the designer go in search for approximation algorithms.


##### Worst, Best and Average Case
 - We have now identified that it is a good idea to express the performance of an algorithm as a function of its input size.
 - However, in some cases, the efficiency of the algorithm also depends of the specifics of a particular input.
 - E.g. for a searching algorithm, if the input is already sorted, then the algorithm is going to perform better than if the list was reversed.


##### Worst Case
 - Given a searching algorithm, one that must linearly search every element, a linear search.
 - For the same input size $n$, the algorithm performs its worst if the target is not within the list or is the last element in the list.
 - In this case, the algorithm makes the largest number of value comparisons.
 - "The worst-case efficiency of an algorithm is its efficiency for the worst-case input of size n, which is an input (or inputs) of size n for which the algorithm runs the longest among all possible inputs of that size." [Levitin 2003]
 - The worst case time complexity is a very important metric as it sets the upper bound.
 - I.e. it guarantees that no matter the specifics of the input of size $n$, it can't be worse than its worst case.


##### Best Case
 - "The best-case efficiency of an algorithm is its efficiency for the best-case input of size n, which is an input (or inputs) of size n for which the algorithm runs the fastest among all the inputs of that size." [Levitin 2003]
 - I.e. it is the opposite of the worst case.
 - E.g. for the searching algorithm mentioned above, the best case would be if the target value was at the first location the algorithm searched.
 - E.g. for the insertion sort algorithm, it performs its best when the list is already sorted, and the best case doesn't degrade by much if the list is almost sorted. Therefore, insertion sort may be good for lists that are known to be almost sorted.
 - If the best case of an algorithm is not good enough, you will need to revisit the approach.


##### Average Case
 - Neither the worst nor best case can answer the question about the running time of a typical or random input.
 - This is given by the average case of an algorithm.








