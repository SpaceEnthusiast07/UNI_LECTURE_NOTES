[Useful website](https://ai.plainenglish.io/gradient-descent-575e81c1a70)
In todays lecture:
 - Linear Regression
 - Gradient Descent
 - Loss/Error Functions


##### Linear Regression
 - Aims to draw one straight line that best goes through all the data points in the given data set
 - Linear Regression = line of best fit (LoBF)
 - Model: $\hat{y}=wx+b$
	 - This means -> "predicted $\hat{y} = weight * x + bias$"
	 - $weight$ = gradient of line
	 - $bias$ = $y$-axis intercept

##### Loss/Error Function
 - We need a method of determining how bad a given regression line is
 - ***Mean Squared Error (MSE)*** -> the average of the squared vertical distance from each point to the regression line, *i.e. the **r** value the calc gives us*
 - Calculation: for every point ($x$), take the difference between the predicated $\hat{y}$ and the actual $y$, square the difference, then take the average of all the squares.
$$MSE(w,b)=\frac{1}{n}\sum_{i=1}^n(\hat{y}_{i}-y_{i})^2=\frac{1}{n}\sum_{i=1}^n(wx_{i}+b-y_{i})^2$$
 - Repeat this for all possible regression lines

##### Gradient Descent - Finding the best line 
![[Pasted image 20251006135008.png]]
 - Every point on the surface of the bowl represents a regression line
 - The height of this point represents the loss/error of that particular regression line
 - The higher up this point is, the larger the loss, the worse the line is
 - Analogy: place a ball randomly within the bowl. Let it roll down hill step by step. Each step check which direction is down hill (the gradient), then move a small amount in that direction (amount is determined by the ***learning rate***)
 - Repeat this process until the ball reaches the bottom (loss is below a threshold)
 - A smaller ***learning rate*** will allow the ball to reach closer to the minima before it goes past it, but this increases the number of calculations






