[Slides](https://ele.exeter.ac.uk/pluginfile.php/5300365/mod_resource/content/4/COM1011-Bayes-and-NaiveBayes.pdf)


##### Resources
 - [3Blue1Brown Video on Bayes Theorem](https://www.youtube.com/watch?v=HZGCoVF3YvM)


##### Contents
 - [[#Conditional Probability]]
 - [[#Bayes' Theorem]]
 - [[#Naive Bayes Classification]]


##### Conditional Probability
 - $P(A|B)$ = "What is the probability A happens *given* that B happened?"
 - Formula:
$$
P(A|B) = \dfrac{P(A \cap B)}{P(B)}
$$
 - Formula says: "Probability of A given B = probability both A and B happen divided by probability B happens"
 - ***Law of Total Probability***: If something can happen in *several different ways*, then the total probability is the *sum* of the *probabilities of each way*
 - Formula:
$$
P(B) = P(B|A_1)P(A_1) + P(B|A_2)P(A_2) + ... + P(B|A_n)P(A_n)
$$
 - Formula says: "To find the overall chance of $B$, take the chance of $B$ in *each possible situation* $A_i$, multiply by how likely that situation is and add them all up"


##### Bayes' Theorem
 - *Idea*: Bayes' Theorem helps you *update a belief* when you get *new evidence*
 - ***Prior***: What you think before
 - ***Evidence***: Something that makes you question the validity of the *prior*
 - ***Posterior***: Bayes tells you what to believe after seeing this evidence
 - Example:
	 - Prior: "I though A was true before"
	 - Evidence: "But now I have just seen B happen"
	 - Posterior: "So what is the new chance A is true?"
 - Formula:
$$
\text{Bayes' Law: } \;\; P(A|B) = \dfrac{P(B|A)P(A)}{P(B)}
$$
![[Pasted image 20251020131751.png]]
 - In words:
	 - New belief = (old belief x how well evidence fits) $\div$ overall chance of evidence being true
![[Pasted image 20251020132854.png]]


##### Naive Bayes Classification
 - 








